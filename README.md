### [Dacon] 모션 키포인트 검출 AI 경진대회 2위 솔루션
- [[대회링크]](https://dacon.io/competitions/official/235701/overview/description/) | [[코드공유]](https://dacon.io/competitions/official/235701/codeshare/2478?page=2&dtype=recent)

<br></br>
### [목차]
  + [문제정의](#문제정의)
  + [데이터](#데이터)
  + [목표](#목표)
  + [학습전략](#학습전략)
  + [시도했지만 효과를 보지 못한 기법](#시도했지만-효과를-보지-못한-기법)


<br></br>
### [문제정의]
- 신체에서 24개의 부위를 추정해야함
  - 흔히 키포인트 디텍션의 기준이 되는 Coco Keypoints 데이터셋은 17개의 키포인트만을 대상으로 함
  
### [데이터]
- `이미지 상에서 사람이 대부분 중앙에 위치함`


- `훈련 데이터`
    - 4195장
    - 사람별 A, B, C, D, E 총 5개의 카메라를 이용해 다른 각도에서 촬영
    - 약 100개 정도의 데이터가 키포인트 매칭이 이상하게 존재 → 수동으로 수정해줌
    

- `테스트 데이터`
    - 1600장
    - 훈련 데이터에는 존재하지 않는 고개 숙인 뒷모습과 같은 포즈가 존재
    - 훈련 기구와 평행하게 놓여진 팔이 자주 등장하고, 기구에 가려진 신체부위가 다수 존재함
    

### [목표]
- 24개의 키포인트 중 17개의 위치는 일치
- 때문에 기존에 존재하는 Keypoint detection SOTA 모델에 추가 학습해 주는 것으로 해결 할 수 있다고 판단
- → `SOTA 모델을 활용해서 해결해보자`
  <br></br>
    - [x] Top-down
        - 영상 이미지에서 사람을 먼저 검출하고, 해당 Bbox내에 존재하는 사람의 포즈를 추정하는 방식
    - [ ] Bottom-up
        - 영상에 포함된 사람의 키포인트를 모두 추정하고 키포인트들간의 상관관계를 분석하여 포즈를 추정하는 방식
    - 일반적으로 `Top-down` 방식이 추론 속도가 느리지만 정확도가 더 좋다고 해서 선택함
  

### [학습전략]
- **[데이터의 특징 활용]**
  - top-down 방식을 위해서 어떤식으로 사람의 위치를 localize할지 생각해봤음 
  - 훈련 데이터
    - keypoints들이 주어진다는 특징을 이용해서 다음 두가지를 시도해봤음
      - [x] 주어진 keyopints들을 활용해서 사람의 중점을 정의하고 해당 중점을 기준으로 affine transformation을 해줬음
      - [ ] 주어진 keypoints들을 기준으로 bbox를 구성해 일반적인 crop을함
      
    <br></br>
    - 실제로 다른 top-down방식의 pose-estimation 모델들을 훈련할 때 bbox의 중점을 통해서 affine transformation을 해준다고함
  <br></br>
  - 테스트 데이터
    - `Yolo-v5`모델을 활용하여 사람의 위치를 localize하고 
      - [ ] 해당 중점을 기준으로 900x900을 뽑아내었음
      - [x] 해당 bbox의 중점을 활용 `서 있는 경우 | 앉아 있는 경우 | 누워 있는 경우`를 파일 이름으로 구분하고 가로 세로의 비율을 다르게 하여 affine transformation 수행을 해줬음 
    - 꼼수이긴 하지만 해당 대회에서의 자세가 위의 세가지 밖에 없었기 때문에 해당 경우들을 나누어서 crop할때의 비율을 바르게 잡아주었음
  <br></br>
  - Transforms
    - 운동 동작을 수행하고 있는 사람의 자세가 데이터임 → 운동에 의한 흐려짐 현상을 해결하고자 `motionblur`, `blur`, `imagecompression`, `gaussianblur`를 사용함
    - 옷의 색상이나, 운동 기구의 색상과 같은 요소에 대한 강인함을 확보하고자 `Channelshffule`, `huesaturation`, `rgbshift`를 사용함


- **[카메라]**
  - 학습 데이터에 각 케이스별로 A, B, C, D, E 카메라가 존재
  - 훈련 및 검증 데이터를 나눌 때에 카메라별로 나눠줬을 때 좀더 밸런스있게 학습할 것이라는 가정하에 시도했었고 → 활용했을때 점수가 소폭 상승했었음
  

- **[추가 데이터]**
  - 포즈의 다양성을 추가해주면 Occlusion에도 강인해질 거라는 가설을 세움
  - Kaggle의 [[New Yoga Pose Dataset]](https://www.kaggle.com/niharika41298/yoga-poses-dataset) 을 활용하여 약 50여개의 데이터를 추가해줌
  

### [시도했지만 효과를 보지 못한 기법]
- **[[AID]](https://arxiv.org/pdf/2008.07139.pdf)**
  - 운동기구에 혹은 카메라 각도에 의해 다른 신체에 의해 신체의 일부가 가려지는 `Occlusion`현상이 자주 보였음
    - 앉아있는 뒷모습의 경우 자주 팔이 운동기구에 가려져서 예측을 못하는 경우가 종종 보임 
    - 고개를 숙이고 서있는 뒷모습의 경우 눈, 코, 귀를 제대로 예측하지 못함
  <br></br>
  - 이를 해결하기 위해서 cutout을 사용해 봤지만 오히려 점수가 하락함
    - cutout은 무작위로 데이터에서 사각 영역을 제거함으로 모델이 물체의 덜 중요한 부분에 집중하지 않게 함으로써 네트워크의 일반화 성능을 끌어올림
    - 하지만 반대로 생각해보면, 랜덤으로 정보를 제거하기 때문에 유의미한 정보를 가진 context information 픽셀들을 너무 많이 제거하여 오히려 키포인트의 위치를 정의하기에 충분하지 않은 정보만을 가질 수 있음
    - 또한 제거하는 영역의 크기 또한 랜덤이기 때문에 너무 작은 영역만을 제거하게 된다면 학습에 유의미한 영향을 주지 못할 수 있음
  <br></br>
  - 그렇게 찾은 방법이 AID(augmentation by information dropping)이라는 기법으로 연속적인 지역의 과도한 삭제 또는 보존을 피하는 것이 information dropping 방식의 핵심 포인트
  - 하지만 성능이 오히려 하락하는 현상이 발생했음 
    - 운동기구에 가려지는 경우는 어느정도 향상됬음 하지만 고개를 숙인 서있는 뒷모습의 경우는 눈과 귀의 예측이 훨씬 엉뚱하게 되어버림
    - 아무래도 훈련 데이터에 없는 자세인데, AID를 통해서 Occlusion에 강하게 학습했고, 때문에 해당 자세를 앞모습이라고 판단하고 뒤에 있는 운동기구를 눈, 코, 귀로 인식하게 되는 것 같음

  
- **[Different Joints Weights]**
  - 뒷모습의 경우 잘 예측하지 못하는 눈, 코, 입이나 운동기구에 가려져서 예측하지 못하는 손목 발목과 같은 부위에 강인하도록 만들기 위해서 해당 키포인트들에 강항 weight를 줘봤음 
    - 큰 효과는 없었고, weight를 낮게 준 몸통이나 팔다리의 경우 오히려 성능이 낮아짐 
    - 또한 고개를 숙인 뒷모습의 경우에 눈, 코, 입의 예측이 괴랄하게 나와서 폐기